# -*- coding: utf-8 -*-
import os
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
from dataset import CrackDataset
from unet_model import UNet
from utils import FocalDiceLoss, calculate_iou

# -------------------------- é…ç½®å‚æ•°ï¼ˆå›ºå®šå­¦ä¹ ç‡ï¼Œæ— è°ƒåº¦ï¼‰--------------------------
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
batch_size = 1  # æŒ‰æ˜¾å­˜è°ƒæ•´ï¼ŒRTX2050 4GBå»ºè®®è®¾ä¸º1
epochs = 60  # æ€»è®­ç»ƒè½®æ¬¡ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹ï¼‰
fixed_lr = 1e-4  # å›ºå®šå­¦ä¹ ç‡ï¼ˆæ— éœ€è°ƒæ•´ï¼‰
patience = 20  # æ—©åœè€å¿ƒå€¼ï¼ˆè¿ç»­8è½®IoUæ— æå‡åˆ™åœæ­¢ï¼‰
model_save_path = "C:/Users/LingZiheng/PycharmProjects/PythonProject/model/best_crack_model.pth"

# æ•°æ®é›†è·¯å¾„ï¼ˆä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„ï¼‰
train_img_dir = "C:/Users/LingZiheng/PycharmProjects/PythonProject/dataset/train/img"
train_mask_dir = "C:/Users/LingZiheng/PycharmProjects/PythonProject/dataset/train/mask"
val_img_dir = "C:/Users/LingZiheng/PycharmProjects/PythonProject/dataset/val/img"
val_mask_dir = "C:/Users/LingZiheng/PycharmProjects/PythonProject/dataset/val/mask"


# -------------------------- æ•°æ®åŠ è½½ --------------------------
def load_data():
    print("\nğŸ“¥ å¼€å§‹åŠ è½½æ•°æ®é›†...")
    # è®­ç»ƒé›†ï¼ˆæ— è¿‡é‡‡æ ·ï¼Œå¦‚éœ€å¼€å¯è®¾oversample=Trueï¼‰
    train_dataset = CrackDataset(
        img_dir=train_img_dir,
        mask_dir=train_mask_dir,
        is_train=True,
        oversample=False
    )
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,
        drop_last=True
    )

    # éªŒè¯é›†
    val_dataset = CrackDataset(
        img_dir=val_img_dir,
        mask_dir=val_mask_dir,
        is_train=False,
        oversample=False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0
    )

    print(f"\nğŸ“Š æ•°æ®åŠ è½½å®Œæˆï¼š")
    print(f"   - è®­ç»ƒé›†ï¼š{len(train_dataset)} æ ·æœ¬ï¼Œ{len(train_loader)} æ‰¹æ¬¡")
    print(f"   - éªŒè¯é›†ï¼š{len(val_dataset)} æ ·æœ¬ï¼Œ{len(val_loader)} æ‰¹æ¬¡")

    # æ— æœ‰æ•ˆæ ·æœ¬æ—¶æŠ¥é”™
    if len(train_dataset) == 0:
        raise ValueError("âŒ è®­ç»ƒé›†æ— æœ‰æ•ˆæ ·æœ¬ï¼è¯·æ£€æŸ¥æ–‡ä»¶ååŒ¹é…å’Œæ–‡ä»¶è·¯å¾„")
    if len(val_dataset) == 0:
        raise ValueError("âŒ éªŒè¯é›†æ— æœ‰æ•ˆæ ·æœ¬ï¼è¯·æ£€æŸ¥æ–‡ä»¶ååŒ¹é…å’Œæ–‡ä»¶è·¯å¾„")

    return train_loader, val_loader


# -------------------------- è®­ç»ƒä¸€è½® --------------------------
def train_one_epoch(model, loader, criterion, optimizer, device):
    model.train()
    total_loss = 0.0
    pbar = tqdm(loader, desc="è®­ç»ƒ")

    for imgs, masks in pbar:
        imgs = imgs.to(device)
        masks = masks.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(imgs)
        loss = criterion(outputs, masks)

        # åå‘ä¼ æ’­+ä¼˜åŒ–ï¼ˆå›ºå®šå­¦ä¹ ç‡ï¼Œæ— è°ƒåº¦ï¼‰
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # ç´¯è®¡æŸå¤±
        total_loss += loss.item() * imgs.size(0)
        pbar.set_postfix({"loss": f"{loss.item():.4f}", "lr": f"{fixed_lr:.6f}"})

    avg_loss = total_loss / len(loader.dataset)
    return avg_loss


# -------------------------- éªŒè¯ä¸€è½® --------------------------
def validate(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0
    total_iou = 0.0
    pbar = tqdm(loader, desc="éªŒè¯")

    with torch.no_grad():
        for imgs, masks in pbar:
            imgs = imgs.to(device)
            masks = masks.to(device)

            outputs = model(imgs)
            loss = criterion(outputs, masks)

            # äºŒå€¼åŒ–ï¼ˆé˜ˆå€¼0.3ï¼Œé€‚é…æµ…ç°è£‚ç¼ï¼‰
            preds = (torch.sigmoid(outputs) > 0.5).float()

            # è®¡ç®—IoU
            batch_iou = 0.0
            for output, target in zip(outputs, masks):  # éå†åŸå§‹è¾“å‡ºoutputså’Œç›®æ ‡mask
                iou = calculate_iou(output.unsqueeze(0), target.unsqueeze(0))  # è¡¥å……é€šé“ç»´åº¦ä»¥åŒ¹é…å‡½æ•°è¦æ±‚
                batch_iou += iou
            batch_avg_iou = batch_iou / len(imgs)

            pbar.set_postfix({"loss": f"{loss.item():.4f}", "IoU": f"{batch_avg_iou:.4f}"})

            total_loss += loss.item() * imgs.size(0)
            total_iou += batch_avg_iou * len(imgs)

    avg_loss = total_loss / len(loader.dataset)
    avg_iou = total_iou / len(loader.dataset)
    return avg_loss, avg_iou


# -------------------------- è®­ç»ƒå¯è§†åŒ– --------------------------
def plot_training_history(train_losses, val_losses, val_ious):
    plt.rcParams['font.sans-serif'] = ['SimHei']
    plt.rcParams['axes.unicode_minus'] = False

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # æŸå¤±æ›²çº¿
    ax1.plot(train_losses, label="è®­ç»ƒæŸå¤±", color="#e74c3c")
    ax1.plot(val_losses, label="éªŒè¯æŸå¤±", color="#3498db")
    ax1.set_xlabel("Epoch")
    ax1.set_ylabel("æŸå¤±å€¼")
    ax1.set_title("è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿ï¼ˆå›ºå®šå­¦ä¹ ç‡ï¼‰")
    ax1.legend()
    ax1.grid(alpha=0.3)

    # IoUæ›²çº¿
    ax2.plot(val_ious, label="éªŒè¯IoU", color="#2ecc71", linewidth=2)
    ax2.set_xlabel("Epoch")
    ax2.set_ylabel("IoUå€¼")
    ax2.set_title("éªŒè¯IoUæ›²çº¿")
    ax2.legend()
    ax2.grid(alpha=0.3)
    ax2.set_ylim(0, 1)

    plt.tight_layout()
    plt.savefig("training_history_fixed_lr.png", dpi=150, bbox_inches="tight")
    plt.show()


# -------------------------- ä¸»è®­ç»ƒæµç¨‹ --------------------------
def main():
    print("=" * 60)
    print("ğŸ”¥ å¼€å§‹è£‚ç¼åˆ†å‰²æ¨¡å‹è®­ç»ƒï¼ˆå›ºå®šå­¦ä¹ ç‡ç‰ˆï¼‰")
    print("=" * 60)
    print(f"ğŸ“Œ é…ç½®ä¿¡æ¯ï¼š")
    print(f"   - è®¾å¤‡ï¼š{device}")
    print(f"   - æ‰¹æ¬¡å¤§å°ï¼š{batch_size}")
    print(f"   - æ€»è½®æ¬¡ï¼š{epochs}")
    print(f"   - å›ºå®šå­¦ä¹ ç‡ï¼š{fixed_lr:.6f}")
    print("=" * 60)

    # 1. åŠ è½½æ•°æ®
    train_loader, val_loader = load_data()

    # 2. åˆå§‹åŒ–æ¨¡å‹ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–å™¨ï¼ˆæ— å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼‰
    model = UNet(n_channels=3, n_classes=1).to(device)
    criterion = FocalDiceLoss(focal_weight=0.4, dice_weight=0.6).to(device)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=fixed_lr,  # ç›´æ¥ä½¿ç”¨å›ºå®šå­¦ä¹ ç‡
        weight_decay=1e-4  # æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
    )

    # 3. è®­ç»ƒè®°å½•ä¸æ—©åœåˆå§‹åŒ–
    train_losses = []
    val_losses = []
    val_ious = []
    best_iou = 0.0
    patience_counter = 0

    # 4. å¼€å§‹è®­ç»ƒï¼ˆæ— å­¦ä¹ ç‡è°ƒæ•´ï¼‰
    for epoch in range(epochs):
        print(f"\nğŸ“Œ Epoch {epoch + 1}/{epochs}")
        print(f"   å½“å‰å­¦ä¹ ç‡ï¼š{fixed_lr:.6f}ï¼ˆå›ºå®šä¸å˜ï¼‰")
        print("-" * 40)

        # è®­ç»ƒä¸éªŒè¯
        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
        val_loss, val_iou = validate(model, val_loader, criterion, device)

        # è®°å½•å†å²æ•°æ®
        train_losses.append(train_loss)
        val_losses.append(val_loss)
        val_ious.append(val_iou)

        # æ‰“å°æœ¬è½®ç»“æœ
        print(f"ğŸ“Š Epoch {epoch + 1} ç»“æœï¼š")
        print(f"   - è®­ç»ƒæŸå¤±ï¼š{train_loss:.4f}")
        print(f"   - éªŒè¯æŸå¤±ï¼š{val_loss:.4f}")
        print(f"   - éªŒè¯IoUï¼š{val_iou:.4f}")

        # æ—©åœæœºåˆ¶+ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_iou > best_iou:
            best_iou = val_iou
            torch.save(model.state_dict(), model_save_path)
            print(f"ğŸ† ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆIoUï¼š{best_iou:.4f}ï¼‰")
            patience_counter = 0
        else:
            patience_counter += 1
            print(f"âš ï¸  æ—©åœè®¡æ•°å™¨ï¼š{patience_counter}/{patience}")
            if patience_counter >= patience:
                print(f"ğŸ›‘ è¿ç»­{patience}è½®IoUæ— æå‡ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break

    # 5. è®­ç»ƒå®Œæˆå¯è§†åŒ–
    print(f"\n" + "=" * 60)
    print(f"ğŸ‰ è®­ç»ƒç»“æŸï¼æœ€ä½³éªŒè¯IoUï¼š{best_iou:.4f}")
    print(f"ğŸ“ æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„ï¼š{model_save_path}")
    print("=" * 60)
    plot_training_history(train_losses, val_losses, val_ious)


if __name__ == "__main__":
    main()